{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import argparse\n",
    "from torch.optim import SGD, Adam\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **config_kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n",
    "        logger.info(\"Initilazing BaseModel\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #save hyperparameters to checkpoint\n",
    "        self.step_count = 0\n",
    "        self.output_dir = Path(self.hparams.output_dir)\n",
    "        self.model = self._load_model()\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def _load_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()), prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input = self.batch2input(batch)\n",
    "        labels = input['labels']\n",
    "        loss, pred_labels, _ = self(**input)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.accuracy(pred_labels.view(-1), labels.view(-1).int()))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        # optimizer = SGD(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            self.train_loader = self.get_dataloader(\"train\", self.hparams.train_batch_size, shuffle=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(\"dev\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader(\"test\", self.hparams.eval_batch_size, shuffle=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_generic_args(parser, root_dir) -> None:\n",
    "        parser.add_argument(\n",
    "            \"--max_epochs\",\n",
    "            default=10,\n",
    "            type=int,\n",
    "            help=\"The number of epochs to train your model.\",\n",
    "        )\n",
    "        ############################################################\n",
    "        ## WARNING: set --gpus 0 if you do not have access to GPUS #\n",
    "        ############################################################\n",
    "        parser.add_argument(\n",
    "            \"--gpus\",\n",
    "            default=1,\n",
    "            type=int,\n",
    "            help=\"The number of GPUs allocated for this, it is by default 1. Set to 0 for no GPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--output_dir\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "        )\n",
    "        parser.add_argument(\"--do_train\", action=\"store_true\", default=True, help=\"Whether to run training.\")\n",
    "        parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "        parser.add_argument(\n",
    "            \"--data_dir\",\n",
    "            default=\"./\",\n",
    "            type=str,\n",
    "            help=\"The input data dir. Should contain the training files.\",\n",
    "        )\n",
    "        parser.add_argument(\"--learning_rate\", default=1e-2, type=float, help=\"The initial learning rate for training.\")\n",
    "        parser.add_argument(\"--num_workers\", default=16, type=int, help=\"kwarg passed to DataLoader\")\n",
    "        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=3, type=int)\n",
    "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
    "\n",
    "def generic_train(\n",
    "    model: BaseModel,\n",
    "    args: argparse.Namespace,\n",
    "    early_stopping_callback=False,\n",
    "    extra_callbacks=[],\n",
    "    checkpoint_callback=None,\n",
    "    logging_callback=None,\n",
    "    **extra_train_kwargs\n",
    "):\n",
    "\n",
    "    # init model\n",
    "    odir = Path(model.hparams.output_dir)\n",
    "    odir.mkdir(exist_ok=True)\n",
    "    log_dir = Path(os.path.join(model.hparams.output_dir, 'logs'))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Tensorboard logger\n",
    "    pl_logger = pl_loggers.TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        version=\"version_\" + datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\"),\n",
    "        name=\"\",\n",
    "        default_hp_metric=True\n",
    "    )\n",
    "\n",
    "    # add custom checkpoints\n",
    "    ckpt_path = os.path.join(\n",
    "        args.output_dir, pl_logger.version, \"checkpoints\",\n",
    "    )\n",
    "    if checkpoint_callback is None:\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=ckpt_path, filename=\"{epoch}-{val_acc:.2f}\", monitor=\"val_acc\", mode=\"max\", save_top_k=1, verbose=True\n",
    "        )\n",
    "\n",
    "    train_params = {}\n",
    "\n",
    "    train_params[\"max_epochs\"] = args.max_epochs\n",
    "\n",
    "    if args.gpus > 1:\n",
    "        train_params[\"distributed_backend\"] = \"ddp\"\n",
    "\n",
    "    trainer = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        enable_model_summary=False,\n",
    "        callbacks= [checkpoint_callback] + extra_callbacks,\n",
    "        logger=pl_logger,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "        # track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
    "        pl_logger.log_hyperparams(params=model.hparams, metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
    "        pl_logger.save()\n",
    "\n",
    "        # save best model to `best_model.ckpt`\n",
    "        target_path = os.path.join(ckpt_path, 'best_model.ckpt')\n",
    "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} to {target_path}.\")\n",
    "        shutil.copy(checkpoint_callback.best_model_path, target_path)\n",
    "\n",
    "\n",
    "    # Optionally, predict on test set and write to output_dir\n",
    "    if args.do_predict:\n",
    "        best_model_path = os.path.join(ckpt_path, \"best_model.ckpt\")\n",
    "        model = model.load_from_checkpoint(best_model_path)\n",
    "        return trainer.test(model), trainer.validate(model)\n",
    "\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "\n",
    "class BERTSequentialMetaphorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Using dataset to process input text on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 50 # assigned based on length analysis of training set\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        note = []\n",
    "        label, text = int(self.data[index][0]), self.data[index][1]\n",
    "        return text, label\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        texts, labels = list(zip(*batch_data))\n",
    "        encodings = self.tokenizer(list(texts), padding=True, truncation=True, max_length=self.max_len, return_tensors= 'pt')\n",
    "        return (\n",
    "                encodings['input_ids'],\n",
    "                encodings['attention_mask'],\n",
    "                torch.LongTensor(labels).view(-1,1)\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class BERT_PL(BaseModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            self.hparams.model_name,\n",
    "            num_labels=2,\n",
    "        )\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.hparams.model_name, config=model_config)\n",
    "\n",
    "    def forward(self, **args):\n",
    "        outputs = self.model(**args)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, predicted_labels, []\n",
    "\n",
    "    def get_dataloader(self, type_path, batch_size, shuffle=False):\n",
    "        # todo add dataset path\n",
    "        datapath = os.path.join(self.hparams.data_dir, f\"sst2.{type_path}\")\n",
    "        data = open(datapath).readlines()\n",
    "        data = [d.strip().split(\" \", maxsplit=1) for d in data] # list of [label, text] pair\n",
    "        dataset = BERTSST2Dataset(self.tokenizer, data)\n",
    "\n",
    "        logger.info(f\"Loading {type_path} data and labels from {datapath}\")\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            collate_fn=dataset.collate_fn\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        optimizer = Adam(model.parameters(), lr=self.hparams.learning_rate)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return {\"input_ids\": batch[0], \"labels\": batch[2], \"attention_mask\": batch[1]}\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser, root_dir):\n",
    "        parser.add_argument(\n",
    "            \"--model_name\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Pretrained tokenizer name or path\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--optimizer\",\n",
    "            default=\"adam\",\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Whether to use SGD or not\",\n",
    "        )\n",
    "        return parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "import time\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    ########################################################\n",
    "    ## TODO: change args if needed according to your files #\n",
    "    ########################################################\n",
    "    mock_args = f\"--data_dir {DATA_DIR} --output_dir bert --optimizer adam \\\n",
    "    --model_name distilbert-base-uncased --learning_rate 0.00005 --max_epochs 2 --do_predict\" # change model_name here\n",
    "\n",
    "    # load hyperparameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    BaseModel.add_generic_args(parser, os.getcwd())\n",
    "    parser = BERT_PL.add_model_specific_args(parser, os.getcwd())\n",
    "    args = parser.parse_args(mock_args.split())\n",
    "    print(args)\n",
    "    # fix random seed to make sure the result is reproducible\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    # If output_dir not provided, a folder will be generated in pwd\n",
    "    if args.output_dir is None:\n",
    "        args.output_dir = os.path.join(\n",
    "            \"./results\",\n",
    "            f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",\n",
    "        )\n",
    "        os.makedirs(args.output_dir)\n",
    "    dict_args = vars(args)\n",
    "    model = BERT_PL(**dict_args)\n",
    "    trainer = generic_train(model, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}